# Cvor
# Optimizing Gradient through CVor: A Universal Control Variates Operator

##  Abstract

Exact gradient estimation is crucial to deep learning training, yet the batch-based updating and nonstationarity of training data present challenges in achieving fast and stable improvements due to the high variance of estimated gradients.Control variates are a promising strategy for variance reduction in gradient estimation, yet their implementation faces significant challenges.In this paper, we present a novel control variates operator, called CVor, which eliminates the need for explicit computing the intermediate Î¸-specific variable c_Î¸(x), thus reducing computational and storage costs. CVor provides low-variance, unbiased gradient estimation for any-order derivatives. By acting on the objective function ğ“›_Î¸ via â„‹(x), CVor transforms the gradient mapping of ğ“›_Î¸ from âˆ‡_Î¸ ğ“›_Î¸ to the control variatesâ€™ structure âˆ‡_Î¸ ğ“›_Î¸ - Î±(âˆ‡_Î¸ â„‹(x) - âˆ‡_Î¸ ğ“—Ìƒ(x)), where âˆ‡_Î¸ â„‹(x) is the primary function of c_Î¸(x). Our theoretical derivation confirms the unbiasedness and higher-order differentiability of CVor. Tested across various variational autoencoder tasks and reinforcement learning benchmarks, CVor offers reduced variance and enhances convergence without incurring additional memory overhead.

## Method

### The stochastic computation graph of loss with CVor

![image-method](/pictures/image-method.png)

## Experiments

Detailed introduction to VAE experiment and SAC experiment code in the corresponding folders

## Results

### Some experimental results are as follows:

![image-variance](pictures/image-variance.png)

Memory usage comparison with different batch sizes

![image-memory](pictures/image-memory.png)

Time complexity comparison with different batch sizes

![image-time](pictures/image-time.png)

## Acknowledgement

The code is modified from Discrete Stein(https://github.com/thjashin/rodeo) and  SAC(https://github.com/pranz24/pytorch-soft-actor-critic)
